---title: "Docker Best Practices: Writing Optimized and Secure Dockerfiles"date: 2022-10-05 11:20:00tags: Docker, Containerization, DevOps, Best Practices---Docker has revolutionized how we build, ship, and run applications. At the heart of a good container is a well-crafted `Dockerfile`. While it's easy to get an application running in a container, creating an image that is small, secure, and fast requires following a set of best practices. This guide covers the essentials for writing optimized and secure Dockerfiles.

<!--more-->

### 1. Use a Specific Base ImageAlways avoid using the `latest` tag for your base images. The `latest` tag is mutable and can point to different versions of an image over time, leading to unpredictable builds. Instead, use a specific version tag.**Bad:```dockerfileFROM python```**Good:```dockerfileFROM python:3.10-slim-bullseye```Furthermore, prefer minimal base images like `slim` or `alpine`. A smaller base image reduces the attack surface and decreases the final image size, leading to faster downloads and deployments.### 2. Leverage the Build Cache with Smart LayeringDocker builds images in layers. Each instruction in a `Dockerfile` creates a new layer. Docker will reuse layers from its cache if the instruction and the files it depends on haven't changed.To take advantage of this, structure your `Dockerfile` to put the things that change least often at the top. For a typical application, this means copying your dependency manifest files (`requirements.txt`, `package.json`, etc.) and installing dependencies *before* copying your application source code.**Bad (Source code and dependencies are in the same layer):**```dockerfileWORKDIR /appCOPY . .RUN pip install -r requirements.txtCMD ["python", "app.py"]```In this example, every time you change a single line of your source code, Docker has to re-run `pip install`, which can be very slow.**Good (Dependencies are in a separate, cached layer):**```dockerfileWORKDIR /app# Copy dependency file firstCOPY requirements.txt .# Install dependenciesRUN pip install -r requirements.txt# Now copy the rest of the source codeCOPY . .CMD ["python", "app.py"]```With this structure, the `pip install` layer is only rebuilt if `requirements.txt` changes.### 3. Use Multi-Stage BuildsA multi-stage build is the most effective way to create a small and secure production image. It allows you to use one image with a full build environment (compilers, development headers, etc.) to compile your code or build assets, and then copy only the necessary artifacts into a lean, final image.This is perfect for compiled languages like Go, Rust, or Java, but it's also useful for frontend applications that need a Node.js environment to build static assets.**Example for a Go application:**```dockerfile# --- Build Stage ---FROM golang:1.19-alpine AS builderWORKDIR /appCOPY . .# Build the Go binaryRUN go build -o my-app# --- Final Stage ---FROM alpine:latestWORKDIR /app# Copy only the compiled binary from the builder stageCOPY --from=builder /app/my-app .# Run the applicationCMD ["./my-app"]```The final image contains only the tiny `alpine` base and our compiled binary, not the entire Go toolchain.### 4. Run as a Non-Root UserBy default, containers run as the `root` user. This is a security risk. If an attacker gains control of your application, they will have root privileges inside the container. To mitigate this, create a dedicated user and group, and run the application as that user.```dockerfile# Create a non-root userRUN addgroup -S appgroup && adduser -S appuser -G appgroup# Set the userUSER appuser# ... rest of your Dockerfile```### 5. Minimize the Number of LayersWhile caching is important, creating an excessive number of layers can add overhead. You can chain related `RUN` commands together using the `&&` operator to combine them into a single layer. This is especially useful for commands that create temporary files you don't want in the final image.**Bad:**```dockerfileRUN apt-get updateRUN apt-get install -y curlRUN curl -o file.txt http://example.com/file```**Good:**```dockerfileRUN apt-get update && \    apt-get install -y curl && \    curl -o file.txt http://example.com/file && \    apt-get clean && \    rm -rf /var/lib/apt/lists/*```Notice the cleanup commands (`apt-get clean`, `rm`) are in the same layer, ensuring the downloaded package lists don't bloat the final image.### ConclusionWriting a `Dockerfile` is easy, but writing a *good* `Dockerfile` is a craft. By following these best practices—using specific and minimal base images, optimizing layer caching, leveraging multi-stage builds, and running as a non-root user—you can create container images that are smaller, faster, and more secure.